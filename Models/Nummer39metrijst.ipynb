{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T21:32:58.787116600Z",
     "start_time": "2024-11-20T21:32:58.782117900Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import os\n",
    "import json\n",
    "\n",
    "import matplotlib as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T21:32:59.435602300Z",
     "start_time": "2024-11-20T21:32:59.419601Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get Label data depending on source image index\n",
    "def find_specific_lookup(data, source_image, query_name):\n",
    "    for entry in data:\n",
    "        if entry[\"search_image\"] == source_image:\n",
    "            for template in entry[\"templates\"]:\n",
    "                if template[\"template\"] == query_name:\n",
    "                    return template\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T21:33:00.164109Z",
     "start_time": "2024-11-20T21:32:59.946107700Z"
    }
   },
   "outputs": [],
   "source": [
    "#get image pairs\n",
    "base_path = os.path.dirname(os.getcwd())\n",
    "\n",
    "#label path\n",
    "lbl_path = os.path.join(base_path, 'Data/labels/train_template_matching.json')\n",
    "\n",
    "#source and query images\n",
    "s_img_path = os.path.join(base_path, 'Data/map_train/51.99908_4.373749.png')\n",
    "q_img_path = os.path.join(base_path, 'Data/train_template_matching')\n",
    "\n",
    "#for now source path is constant\n",
    "s_img = mpimg.imread(s_img_path)\n",
    "\n",
    "with open(lbl_path, 'r') as file:\n",
    "    label = json.load(file)\n",
    "\n",
    "images = []\n",
    "data = []\n",
    "\n",
    "# Read images in path\n",
    "for file in os.listdir(q_img_path):\n",
    "    if file.endswith(\".jpg\") or file.endswith(\".png\") or file.endswith(\".jpeg\"):\n",
    "            q_img = mpimg.imread(os.path.join(q_img_path, file))\n",
    "            images.append([q_img[:, :, :3], s_img[:,:,:3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T21:33:01.080071900Z",
     "start_time": "2024-11-20T21:33:01.071069500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Coordinates to pixels in reference to the source image\n",
    "def CoordToPixel(q_cntr_lat, q_cntr_lon, s_cntr_lat, s_cntr_lon):\n",
    "    w = 240         #pixel width\n",
    "    h = 240         #pixel height\n",
    "    s_zoom = 15     #source image zoom (google maps)\n",
    "\n",
    "    parallelMultiplier = math.cos(s_cntr_lat * math.pi / 180)\n",
    "    degreesPerPixelX = 360 / math.pow(2, s_zoom + 8)\n",
    "    degreesPerPixelY = 360 / math.pow(2, s_zoom + 8) * parallelMultiplier\n",
    "\n",
    "    Y = (s_cntr_lat - q_cntr_lat)/degreesPerPixelY + 0.5*h\n",
    "    X = (q_cntr_lon - s_cntr_lon)/degreesPerPixelX + 0.5*w\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "lbl_data_norm = []\n",
    "s_img_all = []\n",
    "q_img_all = []\n",
    "\n",
    "s_cntr_lat, s_cntr_lon = label[0]['search_image_gps']\n",
    "for tracker, i in enumerate(label[0]['templates']):\n",
    "    q_cntr_lat, q_cntr_lon = i['gps_coords']\n",
    "\n",
    "    # Convert images to numpy arrays\n",
    "    q_img = np.array(images[tracker][0])\n",
    "    s_img = np.array(images[tracker][1])\n",
    "\n",
    "    s_img_all.append(s_img)\n",
    "    q_img_all.append(q_img)\n",
    "\n",
    "    lbl_coord = CoordToPixel(q_cntr_lat, q_cntr_lon, s_cntr_lat, s_cntr_lon)\n",
    "    lbl_coord = np.array(lbl_coord)\n",
    "    \n",
    "    lbl_data_norm.append(lbl_coord)\n",
    "\n",
    "s_img_all = np.array(s_img_all)\n",
    "q_img_all = np.array(q_img_all)\n",
    "lbl_data_norm = np.array(lbl_data_norm)\n",
    "\n",
    "# Flatten images for scaling\n",
    "n_samples, height, width, channels = q_img_all.shape\n",
    "\n",
    "q_img_reshape = q_img_all.reshape(n_samples, height * width * channels)\n",
    "s_img_reshape = s_img_all.reshape(n_samples, height * width * channels)\n",
    "\n",
    "# Normalize the images\n",
    "scaler_q = preprocessing.StandardScaler()\n",
    "scaler_q.fit(q_img_reshape)\n",
    "\n",
    "# Fit and transform\n",
    "q_img_norm = scaler_q.transform(q_img_reshape)\n",
    "s_img_norm = scaler_q.transform(s_img_reshape)\n",
    "\n",
    "# Normalize the labels\n",
    "\n",
    "scaler_lbl = preprocessing.StandardScaler()\n",
    "lbl_data_norm = scaler_lbl.fit_transform(lbl_data_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T21:37:23.720249800Z",
     "start_time": "2024-11-20T21:37:23.702251200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the Siamese Neural Network with resnet50 (much to fucking large)\n",
    "#class SiameseNetwork(nn.Module):\n",
    "#    def __init__(self):\n",
    "#        super(SiameseNetwork, self).__init__()\n",
    "#\n",
    "#        resnet = models.resnet50(pretrained=True)\n",
    "#        \n",
    "#        # Remove the last fully connected layer (classification layer)\n",
    "#        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1]) # Remove the final FC layer\n",
    "#\n",
    "#        self.fc1 = nn.Sequential(\n",
    "#            nn.Linear(2048, 1024),  # ResNet50 outputs 2048-d features\n",
    "#            nn.ReLU(inplace=True),\n",
    "#            \n",
    "#            nn.Linear(1024, 240),\n",
    "#            nn.ReLU(inplace=True),\n",
    "#            \n",
    "#            nn.Linear(240, 2)  # Output is a 2D embedding\n",
    "#        )\n",
    "#        \n",
    "#    def forward_once(self, x):\n",
    "#        # Pass input through the feature extractor (ResNet50)\n",
    "#        output = self.feature_extractor(x)\n",
    "#        \n",
    "#        # Flatten the output tensor\n",
    "#        output = output.view(output.size(0), -1)\n",
    "#        \n",
    "#        # Pass through fully connected layers\n",
    "#        output = self.fc1(output)\n",
    "#        return output\n",
    "#\n",
    "#    def forward(self, input1, input2):\n",
    "#        # Pass both inputs through the network\n",
    "#        output1 = self.forward_once(input1)\n",
    "#        output2 = self.forward_once(input2)\n",
    "#\n",
    "#        return output1, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        \n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 48, kernel_size=11, stride=1),  # Output channels reduced from 96 to 48\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "           \n",
    "            nn.Conv2d(48, 128, kernel_size=5, stride=1, padding=2),  # Output channels reduced from 256 to 128\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "\n",
    "            nn.Conv2d(128, 192, kernel_size=3, stride=1, padding=1),  # Output channels reduced from 384 to 192\n",
    "            nn.ReLU(inplace=True),\n",
    "        \n",
    "            nn.Conv2d(192, 128, kernel_size=3, stride=1, padding=1),  # Output channels reduced from 256 to 128\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "        )\n",
    "\n",
    "        # Fully connected layers for each branch\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(15488, 512),  # Input size adjusted for reduced output dimensions\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "        )\n",
    "        \n",
    "        # Output layer combining the features of both branches\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(512 * 2, 2)  # Adjusted for reduced fc1 output size\n",
    "        )\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        # Forward pass for one input\n",
    "        x = self.cnn1(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # Forward pass for both inputs\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        \n",
    "        # Concatenate the features from both branches\n",
    "        combined = torch.cat((output1, output2), dim=1)\n",
    "        out = self.fc2(combined)  # Final output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuclidianLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(EuclidianLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # euclidian distance\n",
    "        diff = x - y\n",
    "        dist_sq = torch.sum(torch.pow(diff, 2), 1)\n",
    "        loss = torch.sqrt(dist_sq)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T21:37:26.108103700Z",
     "start_time": "2024-11-20T21:37:25.912104300Z"
    }
   },
   "outputs": [],
   "source": [
    "model = SiameseNetwork()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0005 )\n",
    "criterion = EuclidianLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T21:38:51.510716Z",
     "start_time": "2024-11-20T21:38:51.495718600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Custom Dataset Class (similar to the tutorial)\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, q_imgs, s_imgs, labels):\n",
    "        self.q_imgs = q_imgs\n",
    "        self.s_imgs = s_imgs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert images and labels to tensors\n",
    "        q_img = torch.tensor(self.q_imgs[idx], dtype=torch.float32)  # Adjust dtype as needed\n",
    "        s_img = torch.tensor(self.s_imgs[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        \n",
    "        return q_img, s_img, label\n",
    "\n",
    "dataset = SiameseDataset(q_img_norm, s_img_norm, lbl_data_norm)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 1, 11, 11]) - 5808 parameters\n",
      "torch.Size([48]) - 48 parameters\n",
      "torch.Size([128, 48, 5, 5]) - 153600 parameters\n",
      "torch.Size([128]) - 128 parameters\n",
      "torch.Size([192, 128, 3, 3]) - 221184 parameters\n",
      "torch.Size([192]) - 192 parameters\n",
      "torch.Size([128, 192, 3, 3]) - 221184 parameters\n",
      "torch.Size([128]) - 128 parameters\n",
      "torch.Size([512, 15488]) - 7929856 parameters\n",
      "torch.Size([512]) - 512 parameters\n",
      "torch.Size([2, 1024]) - 2048 parameters\n",
      "torch.Size([2]) - 2 parameters\n",
      "Total number of parameters: 8534690\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8534690"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def print_model_size(model):\n",
    "    total_params = 0\n",
    "    for param in model.parameters():\n",
    "        param_count = param.numel()  # Get the number of elements in the parameter tensor\n",
    "        total_params += param_count\n",
    "        print(f\"{param.size()} - {param_count} parameters\")\n",
    "    \n",
    "    print(f\"Total number of parameters: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "# Example usage\n",
    "print_model_size(model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-20T21:47:56.795090600Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "counter = []\n",
    "loss_history = [] \n",
    "iteration_number= 0\n",
    "tracker = 0\n",
    "loss_contrastive = 0\n",
    "# Iterate throught the epochs\n",
    "for epoch in range(20):\n",
    "    \n",
    "    # Iterate over batches\n",
    "    for i, tup in enumerate(dataloader,0):\n",
    "\n",
    "        img0, img1, label = tup\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Pass in the two images into the network and obtain two outputs\n",
    "        output1, output2 = model(img0, img1)\n",
    "\n",
    "        # Pass the outputs of the networks and label into the loss function\n",
    "        loss_contrastive = criterion(output1, output2, label)\n",
    "\n",
    "        # Calculate the backpropagation\n",
    "        loss_contrastive.backward()\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "\n",
    "        # Every 10 batches print out the loss\n",
    "        if epoch % 2 == 0:\n",
    "            print(f\"Epoch number {epoch}\\n Current loss {loss_contrastive.item()}\\n\")\n",
    "            iteration_number += 2\n",
    "        \n",
    "            counter.append(iteration_number)\n",
    "            loss_history.append(loss_contrastive.item())\n",
    "            #print(f\"Epoch number {epoch}\\n Current loss {loss_contrastive.item()}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plotting data\n",
    "def show_plot(iteration,loss):\n",
    "    plt.plot(iteration,loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'counter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m show_plot(\u001b[43mcounter\u001b[49m, loss_history)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'counter' is not defined"
     ]
    }
   ],
   "source": [
    "show_plot(counter, loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "filename = now.strftime(\"resnet50_%d-%m-%Y_%H-%M\")\n",
    "\n",
    "directory  = os.path.join(os.getcwd(), filename+'.txt')\n",
    "\n",
    "model_dir = os.path.join(os.getcwd(), filename+'.h5')\n",
    "\n",
    "model.save(model_dir)\n",
    "\n",
    "comment = \"\"\"\n",
    "Siamese CNN trained on templated and source images\n",
    "Outputs of siamese CNN are fully connected to linear layer\n",
    "Output in pixels\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(os.getcwd(), filename+'.txt'), 'w') as f:\n",
    "    f.write(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
